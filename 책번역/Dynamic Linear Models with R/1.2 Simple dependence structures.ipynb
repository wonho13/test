{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Simple dependence structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시간이력 해석에서 가장 주요한 목적은 예측입니다. 단변수 또는 다변수 시간이력은 랜덤 변수 또는 벡터 ($Y_t : t=1,2,\\dots$, 여기서 $t$는 시간)에 의해 확률적으로 기술됩니다. 단순한 경우로 시간이 균일하게 분포된 경우를 생각해 봅시다. 예를들어 $Y_t$는 $m$ 채권의 그날 가격 또는 제품의 월별 판매가로 생각할 수 있습니다. n개의 관측치 $Y_{1:n}=y_{1:n}$이 주어졌을 때 $n+1$에서의 관측 값 $Y_{n+1}$를 예측하는 것이 문제입니다. 첫번째로 시간이력 간의 연관성을 합리적으로 가정할 필요가 있습니다. 만약 시간이력 $Y_t$에 대한 확률법칙을 결정할 수 있다면 $n\\geq1$에서 결합확률밀도 $\\pi(y_1,\\dots,y_n)$을 알수 있고, 다음과 같이 예측 밀도를 계산하여 베이지안 예측을 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi(y_{n+1}|y_{1:n})=\\frac{\\pi(y_{1:n+1})}{\\pi(y_{1:n})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제로는 결확확률밀도 $\\pi(y_1,\\dots,y_n)$을 직접 결정하는 것은 쉽지 않으며, 변수모델을 적용하여 좀더 쉽게 결정할 수 있습니다. 즉 확률과정을 만드는 특정 변수 $\\theta$를 설정하고 그 $\\theta$에 대한 $(Y_1,\\dots,Y_n)$의 조건부 확률을 표현하는 것이 더 간단할 수 있습니다. 적절한 특성 변수 $\\theta$는 유한 또는 무한 차원일 수 있습니다. 즉 $\\theta$는 랜덤벡터일 수도 있고 상태공간 모델의 경우와 같이 확률과정 그 자체일 수도 있습니다. 연구자들은 종종 $Y_{1:n}$에 대한 조건부 확률밀도 $\\pi(y_{1:n}|\\theta)$를 정의하고, $\\theta$에 대한 분포 $\\pi(\\theta)$를 정의하여 $\\pi(y_{1:n})=\\int \\pi(y_{1:n}|\\theta)\\pi(\\theta)d\\theta$를 통해 $\\pi(y_{1:n})$를 찾아냅니다. 시간이력에 대한 동적 선형모델을 소개할 때 여기서도 동일한 절차를 사용할 것입니다. 우선 좀더 단순한 의존구조에 대해 알아보도록 합시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*조건부 독립*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 단순한 의존 구조는 조건부 독립입니다. 많은 분야에서 $Y_1,\\dots,Y_n$들이 iid (즉, $\\pi(y_{1:n}|\\theta)=\\prod_{i=1}^{n}\\pi(y_i|\\theta)$)라고 가정하는 것은 합리적입니다. 예를들어, $Y_i$가 랜덤 오차의 영향을 받는 반복적인 측정 결과라면, $Y_i=\\theta+\\epsilon_i$이고 $\\epsilon_i$는 측정 장비의 정확성에 따라 평균이 0이고 분산이 $\\sigma^2$인 독립 가우시안 랜덤 오차인 모델로 고려할 수 있습니다. 이는 $\\theta$에 대한 조건부 확률분포 $Y_i$들이 iid라는 것을 의미합니다. 즉, $Y_i|\\theta\\sim N(\\theta,\\sigma^2)$입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "유의해야 할 점은 $Y_1,Y_2,\\dots$들은 단지 조건부 독립이라는 것입니다. 관측치 $y_1,\\dots,y_n$들은 $\\theta$ 값에 대한 정보를 제공해 주며, 이 $\\theta$를 통해 다음 관측치 $Y_{n+1}$를 예측하므로 $Y_{n+1}$는 확률론적 관점에서 과거 관측치들 $Y_1,\\dots,Y_n$들과 연관이 있게 됩니다. 이경우 예측 확률밀도는 다음과 같이 계산할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi(y_{n+1}|y_{1:n})=\\int \\pi(y_{n+1},\\theta|y_{1:n} d\\theta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$=\\int \\pi(y_{n+1}|\\theta,y_{1:n})\\pi(\\theta|y_{1:n}) d\\theta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$=\\int \\pi(y_{n+1}|\\theta)\\pi(\\theta|y_{1:n})d\\theta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 식의 경우 조건부 독립이라는 가정에 의해 결정된 것이며, $\\pi(\\theta|y_{1:n})$는 $(y_1,\\dots,y_n)$에대한  $\\theta$의 조건부 사후 확률밀도입니다. 사후 확률밀도는 베이즈 법칙에 따라 계산할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi(\\theta|y_{1:n})=\\frac{\\pi(y_{1:n}|\\theta)\\pi(\\theta)}{\\pi(y_{1:n})}\\propto\\prod_{t=1}^{n}\\pi(y_t|\\theta)\\pi(\\theta)\\tag{1.1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주변 확률밀도 $\\pi(y_{1:n})$는 $\\theta$와 무관하며 정규화 상수 역할을 합니다. 이에 따라 사후확률밀도는 가능도함수와 사전확률빌도의 곱에 비례하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "흥미로운 점은 조건부 독립이라는 가정이 있으면 사후확률밀도를 순차적(recursively)으로 계산할 수 있다는 것입니다. 이는 모든 이전의 데이터를 저장하고 재가공할 필요가 없다는 것을 의미합니다. 시간 $(n-1)$에서 $\\theta$에 대해 이용가능한 정보는 다음의 조건부확률밀도로 표현됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi(\\theta|y_{1:n-1})\\propto\\prod_{t=1}^{n-1}\\pi(y_t|\\theta)\\pi(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 이 확률밀도함수는 시간 $n$에서의 사전확률밀도함수 역할을 하게 됩니다. 새로운 관측치 $y_n$를 얻으면 베이즈 법칙에 따라 조건부 독립 가정에 따라 $\\pi(y_n|\\theta,y_{1:n-1})=\\pi(y_n|\\theta)$로 표현가능한 가능도 함수를 계산하고, 사전확률밀도함수 $\\pi(\\theta|y_{1:n-1})$를 업데이트하면 됩니다. 이에 따라 다음과 같은 결과를 얻게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi(\\theta|y_{1:n-1},y_n)\\propto\\pi(\\theta|y_{1:n-1})\\pi(y_n|\\theta)\\propto\\prod_{t=1}^{n-1}\\pi(y_t|\\theta)\\pi(\\theta)\\pi(y_n|\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이는 식 (1.1)과 동일합니다. 사후확률밀도의 순차적 구조는 다음 장들에서 소개될 동적 선형모델과 칼만 필터를 학습하는데 있어 중요한 역할을 하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "개념 설명을 위해 단순한 예제를 생각해 봅시다. 한 사람이 바다에서 난파되어 작은 섬에 도착했다고 해 봅시다. $\\theta$는 그 사람의 위치이며 이는 해안에서의 거리라고 합시다. 동적 선형 모델을 학습할 때는 $\\theta$가 시간에 따라 변하는 경우를 생각해 볼 것입니다. (이 사람은 구명 보트 위에 있고 섬에 도착하지 않은 상태 입니다. 따라서 파도를 따라 시간 $t$에서 해안으로부터의 거리 $\\theta_t$ 위치로 이동하는 상황 입니다.) 하지만 이번에는 $\\theta$가 고정된 경우를 생각해 봅시다. 운 좋게도 이 사람은 매 시간마다 해안을 볼 수 있으며 초기 위치 $\\theta$에 대한 정보를 알고 있습니다. 이 사람은 매 시간 관측한 $y_t$로 위치 $\\theta$를 업데이트 하고자 합니다. 베이지안 방법으로 학습 절차를 만들어 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "측정값 $Y_t$는 다음과 같이 모델링할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Y_t=\\theta+\\epsilon_t,\\;\\;\\;\\;\\epsilon_t \\sim iid N(0,\\sigma^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\epsilon_t$들과 $\\theta$는 독립이며 상수 값 $\\sigma^2$을 알고 있다고 합시다. 이를 다시 표현하면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Y_1,Y_2,\\dots,|\\theta \\sim iid N(\\theta,\\sigma^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "($Y_t=\\theta+\\epsilon_t$이므로 평균이 $\\theta$이고 분산이 $\\sigma^2$이 됨.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사전분포를 다음과 같이 설정했다고 합시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta\\sim N(m_0,C_0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기 추정위치 $m_0$에 대한 확신이 적기 때문에 분산 $C_0$를 크게 설정합니다. 측정치 $y_{1:n}$가 주어졌을 때 베이즈 법칙을 사용하여 사후 확률밀도를 개산하고 $\\theta$를 업데이트하게 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\underset{posterior}{\\pi(\\theta|y_{1:n})}\\propto\\underset{likelihood}{\\pi(y_{1:n}|\\theta)}\\underset{prior}{\\pi(\\theta)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "($x\\sim N(\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left\\{-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right\\}$ 이고, $Y_t$는 iid이므로,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$=\\prod_{t=1}^{n}\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left\\{-\\frac{1}{2\\sigma^2}(y_t-\\theta)^2\\right\\}\\;\\;\\frac{1}{\\sqrt{2\\pi C_0}}exp\\left\\{-\\frac{1}{2C_0}(\\theta-m_0)^2\\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\propto exp\\left\\{-\\frac{1}{2\\sigma^2}\\left(\\sum_{t=1}^{n}y_t^2-2\\theta\\sum_{t=1}^n y_t+n\\theta^2\\right)-\\frac{1}{2C_0}(\\theta^2-2\\theta m_0+m_0^2) \\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(위 식에서 $\\theta$가 포함된 항만 정리하면,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\propto exp\\left\\{-\\frac{1}{2\\sigma^2C_0}\\left((nC_0+\\sigma^2)\\theta^2-2(nC_0\\bar{y}+\\sigma^2m_0)\\theta\\right)\\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상기 식은 복잡해 보이지만 사실은 정규분포 확률밀도함수의 커널입니다. $\\theta \\sim N(m,C)$이면 $\\pi(\\theta) \\propto exp\\{-(1/2C)(\\theta^2-2m\\theta)\\}$임을 이용하기 위해, 상기 식을 다음과 같이 변형하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(위 식 또한 정규분포 식을 $\\theta$항만 남긴 것임)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$exp\\left\\{-\\frac{1}{2\\sigma^2C_0/(nC_0+\\sigma^2)}\\left(\\theta^2-2\\frac{nC_0\\bar{y}+\\sigma^2m_0}{(nC_0+\\sigma^2)}\\theta\\right)\\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 다음과 같이 정리할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta|y_{1:n} \\sim N(m_n,C_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$m_n=E(\\theta|y_{1:n})=\\frac{C_0}{C_0+\\sigma^2/n}\\bar{y}+\\frac{\\sigma^2/n}{C_0+\\sigma^2/n}m_0\\tag{1.2a}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$C_n=Var(\\theta|y_{1:n})=\\left(\\frac{n}{\\sigma^2}+\\frac{1}{C_0}\\right)^{-1}=\\frac{\\sigma^2C_0}{\\sigma^2+nC_0}\\tag{1.2b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사후 정밀도(precision)는 $1/C_n=n/\\sigma^2+1/C_0$이며, 이는 초기 정밀도 $1/C_0$와 샘플 평균의 정밀도 $n/\\sigma^2$들의 합입니다. 사후 정밀도는 항상 초기 정밀도보다 큽니다. 데이터의 품질이 좋지 않더라도 어떠한 정보를 제공하기 때문입니다. 사후 평균 $m_n=E(\\theta|y_{1:n})$은 사전 평균 $m_0=E(\\theta)$와 샘플 평균 $\\bar{y}=\\sum_{i=1}^{n} y_i/n$의 가중평균 입니다. 가중치는 $C_0$와 $\\sigma^2$로 결정됩니다. 사전 불확실성의 척도인 사전 공분산 $C_0$가 잡음 공분산 $\\sigma^2$보다 작다면 사전정보가 더 많은 가중치를 갖게 됩니다. 사전 공분산 $C_0$가 매우 크다면 $m_n\\simeq\\bar{y}$, $C_n\\simeq\\sigma^2/n$이 됩니다. (즉, 사전정보의 영향은 거의 없음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 봐 왔듯, 사후 분포는 순차적으로 계산할 수 있습니다. 시간 $n$에서 $y_{1:n-1}$이 주어졌을 때 $\\theta$의 조건부 밀도 $N(m_{n-1},C_{n-1})$는 사전분포 역할을 하며, 현재 관측에 대한 가능도는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi(y_n|\\theta,y_{1:n-1})=\\pi(y_n|\\theta)=N(y_n;\\theta,\\sigma^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "식 (1.2)를 사용하고 $m_{n-1}$, $C_{n-1}$를 $m_n$, $C_n$으로 대체하면 관측치 $y_n$을 사용해 사전분포 $N(m_{n-1},C_{n-1})$를 업데이트할 수 있습니다. 사후밀도 또한 다음의 변수들을 갖는 가우시안 분포임을 알 수 있었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$m_n=\\frac{C_{n-1}}{C_{n-1}+\\sigma^2}y_n+\\left(1-\\frac{C_{n-1}}{C_{n-1}+\\sigma^2}\\right)m_{n-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$=m_{n-1}+\\frac{C_{n-1}}{C_{n-1}+\\sigma^2}(y_n-m_{n-1})\\tag{1.3a}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$C_n=\\left(\\frac{1}{\\sigma^2}+\\frac{1}{C_{n-1}}\\right)^{-1}=\\frac{\\sigma^2C_{n-1}}{\\sigma^2+C_{n-1}}\\tag{1.3b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y_{n+1}=\\theta+\\epsilon_{n+}$이므로, $Y_{n+1}|y_{1:n}$에 대한 예측 분포는 평균이 $m_n$이고 분산이 $C_n+\\sigma^2$인 정규 분포입니다. 따라서 $m_n$은 $\\theta$에 대한 사후 평균 값이며 한 스텝 다음의 점예측 $E(Y_{n+1}|y_{1:n})$입니다. 식(1.3a)는 다음과 같은 가중치를 갖는 예측 오차 $e_n=y_n-m_{n-1}$를 고려하는 이전의 추정치 $m_{n-1}$을 모아 $m_n$을 계산함을 보여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{C_{n-1}}{C_{n-1}+\\sigma^2}=\\frac{C_0}{\\sigma^2+nC_0}\\tag{1.4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2장에서 보겠지만 이 예측 오차 수정 구조는 동적 선형 모델에 대한 칼만필터 식입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*상호교환성*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "베이지안 해석에서 상호교환성은 기본적인 의존 구조 입니다. 랜덤 벡터의 유한한 과정 $(Y_t:t=1,2,\\dots)$를 고려해 봅시다. 이 과정의 순서가 절절하지 않다고 생각해 봅시다. 즉 $n\\geq1$에서 벡터 $(Y_1,\\dots,Y_n)$과 이 성분들을 섞은 $(Y_{i1},\\dots,Y_{in})$의 분포가 같다고 해 봅시다. 이 경우, $(Y_t:t=1,2,\\dots)$는 상호교환가능하다고 합니다. 이러한 가정은 동일한 조건에서 반복된 실험의 결과로 $Y_t$가 산출되었다고 한다면 합리적인 가정입니다. 앞 장에서 고려한 예제에서는 측정한 거리 $Y_t$의 순서를 상호교환가능다하고 하는 것은 부적절 합니다. de Finetti's의 정리에 따르면 앞서 소개한 상호교환가능성은 조건부 iid와 동일한 가정입니다. 하지만 중요한 차이점이 있습니다. 여기서는 관측 가능한 의존 구조에 대한 가정 즉 상호교환성을 가정하였지만, 지금까지는 변수 모델 또는 변수에 대한 사전 분포에 대해서는 소개하지 않았습니다. 사실 가상의 모델 즉 가능도 및 사전모델의 조합은 상호교환성 가정으로 부터 만들어 진 것이며 다음 소개할 정리에서 보여질 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리 1.1 (de Finetti representation 정리)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(Y_t:t=1,2,\\dots)$를 상호교환가능한 랜덤 벡터의 유한한 과정이라고 해 봅시다. 그러면 다음을 만족합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $n\\to\\infty$이면, 1의 확률로 다음의 경험적 분포함수는 랜덤 분포 함수 F로 약하게 수렴합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F_n(y)=F_n(y;Y_1,\\dots,Y_n)=\\frac{1}{n}\\sum_{i=1}^{n}I_{-\\infty,y}(Y_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. $n\\geq1$인 경우, $(Y_1,\\dots,Y_n)$의 분포 함수는 다음과 같이 표현될 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(Y_1\\leq y_1,\\dots,Y_n\\leq y_n)=\\int\\prod_{i=1}^{n}\\pi(y_i)d\\pi(F)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 $\\pi$는 경험적 분포함수 과정에 대한 약한 제한 F 확률 법칙 입니다.(??)\n",
    "\n",
    "where $\\pi$ is the probability law of the weak limit F of the sequence of the empirical distribution functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
